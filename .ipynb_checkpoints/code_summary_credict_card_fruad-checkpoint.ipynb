{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eaabed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# StandScaler 적용\n",
    "\n",
    "# 로그 변환 적용\n",
    "# 이상치 제거 적용\n",
    "\n",
    "# 오버샘플링 적용 \n",
    "# 업무 특징상 재현율을 높여야 하는 경우가 있는데, 그때 오버 샘플링을 사용\n",
    "# 재현율이 보통 중요한 지표 (꼭 그런 것은 아니지만)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685d5250",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################\n",
    "# 심하게 불균형 되어 있는 데이터 셋\n",
    "# 284,807건의 데이터 중 492건이 Fraud임. 전체의 0.172% \n",
    "\n",
    "# feature engineering 방식을 차례로 Logistic Regression과 LightGBM을 이용하여 적용 후 비교\n",
    "# 1. 중요 feature의 데이터 분포도 변경 (정규분포, log변환)\n",
    "# 2. 이상치 제거 (outlier)\n",
    "# 3. SMOTE 오버 샘플링 \n",
    "\n",
    "# 4. Logistic Regression (이진 classification)\n",
    "# 5. LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cb7795",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################\n",
    "# Log 변환\n",
    "# 왜곡된 분포도를 가진 데이터 세트를 비교적 정규 분포에 가깝게 변환해주는 훌륭한 Feature Engineering 방식\n",
    "\n",
    "# IQR(Inter Quantile Range)를 이용한 Outlier Removal\n",
    "# 이상치 : Q3 +1.5*IQR (최댓값), Q1 - 1.5*IQR (최솟값)의 범위를 넘어갈 때 \n",
    "# IQR = Q3 - Q1\n",
    "\n",
    "# 언더 샘플링(undersampling), 오버샘플링 (oversampling)\n",
    "# 1. 레이블이 불균형한 분포를 가진 데이터 학습 시, 이상 레이블을 가진 데이터 건수가 매우 적어 제대로 된 유형의 학습이 어려움..\n",
    "# 2. 반면에 정상 레이블을 가지는 데이터 건수는 매우 많아 일방적으로 정상 레이블로 치우친 학습을 수행하여, 제대로된 이상 데이터 검출이 어려움.\n",
    "# 3. 대표적으로 oversampling과 undersampling이 있음.\n",
    "# oversampling -> 적은 레이블을 가진 데이터 세트를 많은 레이블을 가진 데이터 세트 수준으로 증식.\n",
    "# undersampling -> 많은 레이블을 가진 데이터 세트를 적은 레이블 세트를 가진 데이터 세트 수준으로 감소 샘플링.\n",
    "\n",
    "# SMOTE(Synthetic Minority Over-sampling Technique) 개요\n",
    "# 1. k 최근접 이웃으로 데이터 신규 증식\n",
    "# 2. 신규 증식하여 오버 샘플링 완료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad99571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예제 데이터\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline\n",
    "\n",
    "card_df = pd.read_csv('./creditcard.csv')\n",
    "card_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdf2051",
   "metadata": {},
   "outputs": [],
   "source": [
    "### amount에 standardscaler 적용 ###\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 사전 feature engineering 함수\n",
    "def get_preprocessed_df(df=None):\n",
    "    df_copy = df.copy()\n",
    "    scaler = StandardScaler()                         # ndarray -> 2차원\n",
    "    amount_n = scaler.fit_transform(df_copy['Amount'].values.reshape(-1,1))\n",
    "    # 변환된 Amount를 Amount_Scaled로 피처명 변경후 DataFrame맨 앞 컬럼으로 입력\n",
    "    df_copy.insert(0, 'Amount_Scaled', amount_n)\n",
    "    # 기존 Time, Amount 피처 삭제\n",
    "    df_copy.drop(['Time', 'Amount'], axis=1, inplace=True)\n",
    "    return df_copy\n",
    "\n",
    "# train, test 데이터 셋을 반환하는 함수\n",
    "def get_train_test_dataset(df=None):\n",
    "    df_copy = get_preprocessed_df(df)\n",
    "    \n",
    "    X_features = df_copy.iloc[:, :-1]\n",
    "    y_target = df_copy.iloc[:, -1]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_features, y_target, test_size=.3, random_state=0,\n",
    "                                                        stratify=y_target) # stratify 꼭 적어주기\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_train_test_dataset(card_df)\n",
    "\n",
    "# 모델을 학습/예측/평가하는 함수\n",
    "def get_model_train_eval(model, ftr_train=None, ftr_test=None, tgt_train=None, tgt_test=None):\n",
    "    model.fit(ftr_train, tgt_train)\n",
    "    pred = model.predict(ftr_test)\n",
    "    pred_proba = model.predict_proba(ftr_test)[:, 1]\n",
    "    get_clf_eval(tgt_test, pred, pred_proba)\n",
    "\n",
    "# 로지스틱 회귀 성능 예측\n",
    "lr_clf = LogisticRegression(max_iter=1000)\n",
    "get_model_train_eval(lr_clf, ftr_train = X_train, ftr_test = X_test, tgt_train = y_train, tgt_test = y_test)\n",
    "\n",
    "# lightGBM 예측 성능\n",
    "# class가 극도로 불균형 하기 때문에 boost_from_average=False로 지정\n",
    "lgbm_clf = LGBMClassifier(n_estimators=1000, num_leaves=64, n_jobs=-1, boost_from_average=False)\n",
    "get_model_train_eval(lgbm_clf, ftr_train = X_train, ftr_test = X_test, tgt_train = y_train, tgt_test = y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59208dde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74813fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### amount를 로그변환 ###\n",
    "\n",
    "def get_preprocessed_df(df=None):\n",
    "    df_copy = df.copy()\n",
    "    # numpy의 log1p()를 이용하여 amount를 로그변환 -> log(x + 1)\n",
    "    # 무한대의 값이 안나오도록 하기 위하여 log1p를 사용함\n",
    "    amount_n = np.log1p(df_copy['Amount'])\n",
    "    df_copy.insert(0, 'Amount_Scaled', amount_n)\n",
    "    # 기존 Time, Amount 피처 삭제\n",
    "    df_copy.drop(['Time', 'Amount'], axis=1, inplace=True)\n",
    "    return df_copy\n",
    "\n",
    "def get_train_test_dataset(df=None):\n",
    "    df_copy = get_preprocessed_df(df)\n",
    "    \n",
    "    X_features = df_copy.iloc[:, :-1]\n",
    "    y_target = df_copy.iloc[:, -1]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_features, y_target, test_size=.3, random_state=0,\n",
    "                                                        stratify=y_target) # stratify 꼭 적어주기\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_train_test_dataset(card_df)\n",
    "\n",
    "# 로지스틱 회귀 성능 예측\n",
    "lr_clf = LogisticRegression(max_iter=1000)\n",
    "get_model_train_eval(lr_clf, ftr_train = X_train, ftr_test = X_test, tgt_train = y_train, tgt_test = y_test)\n",
    "\n",
    "# lightGBM 예측 성능\n",
    "lgbm_clf = LGBMClassifier(n_estimators=1000, num_leaves=64, n_jobs=-1, boost_from_average=False)\n",
    "get_model_train_eval(lgbm_clf, ftr_train = X_train, ftr_test = X_test, tgt_train = y_train, tgt_test = y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6bfe96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aca1f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 이상치 제거 함수. \n",
    "def get_outlier(df=None, column=None, weight=1.5):\n",
    "    # fraud에 해당하는 column 데이터만 추출, 1/4 분위와 3/4 분위 지점을 np.percentile로 구함. \n",
    "    fraud = df[df['Class']==1][column]\n",
    "    quantile_25 = np.percentile(fraud.values, 25)\n",
    "    quantile_75 = np.percentile(fraud.values, 75)\n",
    "    # IQR을 구하고, IQR에 1.5를 곱하여 최대값과 최소값 지점 구함. \n",
    "    iqr = quantile_75 - quantile_25\n",
    "    iqr_weight = iqr * weight\n",
    "    lowest_val = quantile_25 - iqr_weight\n",
    "    highest_val = quantile_75 + iqr_weight\n",
    "    # 최대값 보다 크거나, 최소값 보다 작은 값을 아웃라이어로 설정하고 DataFrame index 반환. \n",
    "    outlier_index = fraud[(fraud < lowest_val) | (fraud > highest_val)].index\n",
    "    return outlier_index\n",
    "\n",
    "\n",
    "# 이상치 데이터 제거 후 모델 학습/예측/평가\n",
    "# 각 피처들의 상관 관계를 시각화, 결정 레이블인 class 값과 가장 상관도가 높은 피처 추출\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "corr = card_df.corr()\n",
    "sns.heatmap(corr, annot=True, fmt='.1f',  cmap='RdBu')\n",
    "\n",
    "\n",
    "# get_processed_df( )를 로그 변환 후 V14 피처의 이상치 데이터를 삭제하는 로직으로 변경. \n",
    "# 이상치를 너무 많이 제거하면 성능이 떨어질 수 있음.\n",
    "# 이상치를 기준에서만 제거\n",
    "\n",
    "def get_preprocessed_df(df=None):\n",
    "    df_copy = df.copy()\n",
    "    amount_n = np.log1p(df_copy['Amount'])\n",
    "    df_copy.insert(0, 'Amount_Scaled', amount_n)\n",
    "    df_copy.drop(['Time','Amount'], axis=1, inplace=True)\n",
    "    # 이상치 데이터 삭제하는 로직 추가\n",
    "    outlier_index = get_outlier(df=df_copy, column='V14', weight=1.5)\n",
    "    df_copy.drop(outlier_index, axis=0, inplace=True)\n",
    "    return df_copy\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_train_test_dataset(card_df)\n",
    "print('### 로지스틱 회귀 예측 성능 ###')\n",
    "get_model_train_eval(lr_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)\n",
    "print('### LightGBM 예측 성능 ###')\n",
    "get_model_train_eval(lgbm_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ba5892",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b212849d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 오버 샘플링 ##\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(random_state=0)\n",
    "# over_sampling된 값 반환\n",
    "X_train_over, y_train_over = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# 하지만 결괏값에서 정밀도가 굉장히 낮음.\n",
    "# 재현율은 높음. \n",
    "# 로짓\n",
    "lr_clf = LogisticRegression(max_iter=1000)\n",
    "get_model_train_eval(lr_clf, ftr_train=X_train_over, ftr_test = X_test, tgt_train=y_train_over, tgt_test=y_test)\n",
    "# lightGBM\n",
    "lgbm_clf = LGBMClassifier(n_estimators=1000, num_leaves=64, n_jobs=-1, boost_from_average=False)\n",
    "get_model_train_eval(lgbm_clf, ftr_train=X_train_over, ftr_test=X_test,\n",
    "                  tgt_train=y_train_over, tgt_test=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2963f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "%matplotlib inline\n",
    "\n",
    "def precision_recall_curve_plot(y_test , pred_proba_c1):\n",
    "    # threshold ndarray와 이 threshold에 따른 정밀도, 재현율 ndarray 추출. \n",
    "    precisions, recalls, thresholds = precision_recall_curve( y_test, pred_proba_c1)\n",
    "    \n",
    "    # X축을 threshold값으로, Y축은 정밀도, 재현율 값으로 각각 Plot 수행. 정밀도는 점선으로 표시\n",
    "    plt.figure(figsize=(8,6))\n",
    "    threshold_boundary = thresholds.shape[0]\n",
    "    plt.plot(thresholds, precisions[0:threshold_boundary], linestyle='--', label='precision')\n",
    "    plt.plot(thresholds, recalls[0:threshold_boundary],label='recall')\n",
    "    \n",
    "    # threshold 값 X 축의 Scale을 0.1 단위로 변경\n",
    "    start, end = plt.xlim()\n",
    "    plt.xticks(np.round(np.arange(start, end, 0.1),2))\n",
    "    \n",
    "    # x축, y축 label과 legend, 그리고 grid 설정\n",
    "    plt.xlabel('Threshold value'); plt.ylabel('Precision and Recall value')\n",
    "    plt.legend(); plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "precision_recall_curve_plot(y_test, lr_clf.predict_proba(X_test)[:,1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
