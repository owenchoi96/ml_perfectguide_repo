{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f599bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예제 데이터 \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "cust_df = pd.read_csv('./train_santander.csv', encoding='latin-1')\n",
    "print('dataset shape: ', cust_df.shape)\n",
    "cust_df.head(3)\n",
    "# 0이 만족 1이 불만족\n",
    "# 여기서 1이 불만족인이유는, 불만족인 데이터를 찾기 위해서"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04ba8c0",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf0878d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 피처 세트와 레이블 세트 분리.\n",
    "X_features = cust_df.iloc[:, :-1]\n",
    "y_labels = cust_df.iloc[:, -1]\n",
    "\n",
    "# 2. 훈련 데이터와 테스트 데이터 분리, 훈련 데이터에서 훈련 데이터, 검증 데이터 분리\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_features, y_train, test_size=.2, \n",
    "                                                    random_state = 0)\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=.3, random_state=0)\n",
    "\n",
    "# 3-1. xgboost 모델로 학습\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Classifier에 하이퍼 파라미터 입력\n",
    "# 성능 평가 지표 -> eval_metric = 'auc' (가능하다면 eval_metric을 명시적으로 적어주는 것이 좋음.)\n",
    "# auc의 경우 predict_proba를 가지고 와야됨.\n",
    "xgb_clf = XGBClassifier(n_estimators=500, learning_rate=.05, random_state=156)\n",
    "xgb_clf.fit(X_tr, y_tr,\n",
    "            early_stopping_rounds=100, eval_metric='auc',\n",
    "            eval_set = [(X_tr, y_tr), (X_val, y_val)])\n",
    "\n",
    "# 마지막에 y_test 데이터를 넣어주어 점수 확인\n",
    "xgb_roc_score = roc_auc_score(y_test, xgb_clf.predict_proba(X_test)[:, 1], \n",
    "                              average='macro')\n",
    "print('ROC AUC: {0 :.4f}'.format(xgb_roc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c767605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-2. hyperopt를 이용하여 하이퍼 파라미터 튜닝\n",
    "from hyperopt import hp\n",
    "\n",
    "# 1. search space\n",
    "# max_depth는 5에서 15까지 1간격으로...\n",
    "# colsample_bytree는 .5에서 .95사이 정규 분포된 값으로 검색...\n",
    "xgb_search_space = {\n",
    "    'max_depth' : hp.quniform('max_depth', 5, 15, 1),\n",
    "    'min_child_weight' : hp.quniform('min_child_weight', 1, 6, 1),\n",
    "    'colsample_bytree' : hp.uniform('colsample_bytree', .5, .95),\n",
    "    'learning_rate' : hp.uniform('learning_rate', .01, .2)\n",
    "}\n",
    "\n",
    "# 2. objective function\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "    # learning_rate: 학습률\n",
    "    # n_estimators: 약한 학습기 개수\n",
    "    ### min_child_weight: 트리에서 추가적으로 분할할지를 결정하기 위해 필요한 weight의 총합 (디폴트는 1, 값이 클수록 분할 자제)\n",
    "    ### max_depth : 트리의 최대 깊이\n",
    "    # subsample : 트리가 커져서 과적합되는 것을 제어하기 위해 데이터를 샘플링하는 비율. 일반적으로.5 ~ 1사이의 값을 씀\n",
    "    # reg_lambda : L2 규제\n",
    "    # reg_alpha : L1 규제\n",
    "    ### colsample_bytree : GBM의 max_features와 유사. (피처 비율을 조절하여 overfitting 방지)\n",
    "    # scale_pos_weight : 특정 값으로 치우친 비대칭한 클래스로 구성된 데이터 세트의 균형을 유지하기 위한 파라미터. (기본값은 1)\n",
    "    # gamma :\n",
    "\n",
    "def objective_func(search_space):\n",
    "    xgb_clf = XGBClassifier(n_estimators=100,\n",
    "                            max_depth = int(search_space['max_depth']),\n",
    "                            min_child_weight = int(searach_space['min_child_weight']),\n",
    "                            colsample_bytree = search_space['colsample_bytree'],\n",
    "                            learning_rate = search_space['learning_rate'])\n",
    "    # 3개 kfold 방식으로 평가된 roc_auc 지표를 담는 list\n",
    "    roc_auc_list = []\n",
    "    \n",
    "    # 3개 폴드 방식\n",
    "    # kfold를 쓰는 이유는 cross_val_score와 다르게 early_stopping_rounds를 사용가능하기 때문\n",
    "    kf = KFold(n_splits=3)\n",
    "    \n",
    "    # 학습과 검증 데이터 셋을 나누어줌\n",
    "    for tr_index, val_index in kf.split(X_train):\n",
    "        # tr_index -> 학습용 인덱스 / val_index -> 검증용 인덱스\n",
    "        X_tr, y_tr = X_train.iloc[tr_index], y_train.iloc[tr_index]\n",
    "        X_val, y_val = X_train.iloc[val_index], y_train.iloc[val_index]\n",
    "        # early stopping은 30회로 설정\n",
    "        xgb_clf.fit(X_tr, y_tr, early_stopping_rounds=30, eval_metric='auc',\n",
    "                    eval_set = [(X_tr, y_tr), (X_val, y_val)])\n",
    "        # score 계산\n",
    "        # 여기서 y_test가 들어가면 안됨 -> 마지막에 쓰임\n",
    "        # 여기서는 y_val이 쓰임\n",
    "        score = roc_auc_score(y_val, xgb_clf.predict_proba(X_val)[:, 1])\n",
    "        roc_auc_list.append(score)\n",
    "       \n",
    "    # -1을 곱해줌 -> fmin에서 최소의 값을 구해야 하기 때문에\n",
    "    return -1 * np.mean(roc_auc_list)\n",
    "\n",
    "# 3. fmin\n",
    "from hyperopt import fmin, tpe, Trials\n",
    "trials = Trials()\n",
    "\n",
    "best = fmin(fn=objective_func, space=xgb_search_space,\n",
    "            algo=tpe.suggest, # 거의 고정\n",
    "            max_evals=50, # 최대 반복 횟수\n",
    "            trials=trials, rstate=np.random.default_rng(seed=30))\n",
    "print('best: ', best)\n",
    "\n",
    "# 4. 최적의 하이퍼 파라미터를 기반으로 학습과 예측\n",
    "xgb_clf = XGBClassifier(n_estimators=500, learning_rate=round(best['learning_rate'], 5),\n",
    "                        max_depth=int(best['max_depth']), min_child_weight = int(best['min_child_weight']),\n",
    "                        colsample_bytree = round(best['colsample_bytree'], 5))\n",
    "xgb_clf.fit(X_tr, y_tr, early_stopping_rounds=100,\n",
    "            eval_metric = 'auc', eval_set = [(X_tr, y_tr), (X_val, y_val)])\n",
    "# 여기서 y_test 데이터를 써서 점수를 구해줌\n",
    "xgb_roc_score = roc_auc_score(y_test, xgb_clf.predict_proba(X_test)[:,1])\n",
    "print('ROC AUC: {0: .4f}'.format(xgb_roc_score))\n",
    "\n",
    "# 5. plot_importance\n",
    "from xgboost import plot_importance\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(10,8))\n",
    "plot_importance(xgb_clf, ax=ax, max_num_features=20, height=.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9d5ffc",
   "metadata": {},
   "source": [
    "# lightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2c3e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### lightGBM ####\n",
    "# num_leaves개수를 중심으로 min_child_samples(min_data_in_leaf), max_depth를 함께 조정하면서 모델의 복잡도를 줄이는 것이 기본 튜닝 방안\n",
    "\n",
    "    # n_estimators\n",
    "    #### learning_rate\n",
    "    #### max_depth\n",
    "    #### min_child_samples\n",
    "    # subsample\n",
    "    #### colsample_bytree : max_features와 유사 (많이 중요함) -> 피처가 많으면 overfitting될 수 있어 피처 개수를 랜덤으로 줄임. \n",
    "    # reg_lambda\n",
    "    # reg_alpha\n",
    "    # early_stopping_rounds\n",
    "    ##### num_leaves : lightGBM에서는 기반이 되는 하이퍼 파라미터\n",
    "    # min_child_weight\n",
    "    \n",
    "## 트리구조 ##\n",
    "# max_depth\n",
    "# num_leaves\n",
    "# min_child_samples\n",
    "# min_child_weight\n",
    "\n",
    "## 샘플링 비율 ##\n",
    "# subsample : 서브 데이터의 비율\n",
    "# colsample_bytree : ex)columns이 100개이면 그 중에서 50개만 한다는 가 (비율)\n",
    "\n",
    "## 손실함수 규제 ##\n",
    "# reg_lambda\n",
    "# reg_alpha\n",
    "\n",
    "## learning rate ##\n",
    "\n",
    "# 위의 9가지 정도만 정해도 괜찮음. \n",
    "# 너무 많은 하이퍼 파라미터들을 튜닝하려는 것은 오히려 최적값을 찾는데 방해가 될 수 있음.\n",
    "# 적당한 수준의 하이퍼 파라미터 개수 설정 필요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff44cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼 파라미터 튜닝 없이\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "lgmb_clf = LGBMClassifier(n_estimators=500)\n",
    "\n",
    "eval_set = [(X_tr, y_tr), (X_val, y_val)]\n",
    "lgbm_clf.fit(X_tr, y_tr, early_stopping_rounds=100, eval_metric='auc', eval_set = eval_set)\n",
    "\n",
    "lgbm_roc_score = roc_auc_score(y_test, lgbm_clf.predict_proba(X_test)[:,1])\n",
    "print(\"ROC AUC: {0: .4f}\".format(lgbm_roc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba9e9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼 파라미터 튜닝\n",
    "\n",
    "# 1. search_space\n",
    "# lgbm은 num_leaves를 기반으로 과적합을 제어함\n",
    "# 다른 모델이었으면 max_depth가 크면 과적합이 생기기 쉬운데, 여기서는 num_leaves가 기반이기 때문에 괜찮음\n",
    "lgbm_search_space = {\n",
    "    'num_leaves' : hp.quniform('num_leaves', 32, 64, 1),\n",
    "    'max_depth' : hp.quniform('max_depth', 100, 160, 1),\n",
    "    'min_child_samples' : hp.quniform('min_child_samples', 60, 100, 1),\n",
    "    'subsample' : hp.uniform('subsample', 0.7, 1),\n",
    "    'learning_rate' : hp.uniform('learning_rate', .01, .2)\n",
    "}\n",
    "\n",
    "# 2. objective function\n",
    "def objective_func(search_space):\n",
    "    lgbm_clf = LGBMClassifier(n_estimators=100,\n",
    "                              num_leaves = int(search_space['num_leaves']),\n",
    "                              max_depth= int(search_space['max_depth']),\n",
    "                              min_child_samples = int(search_space['min_child_samples']),\n",
    "                              subsample = search_space['subsample'],\n",
    "                              learning_rate = search_space['learning_rate'])\n",
    "    # 3개의 kfold 방식으로 평가된 roc_auc 지표를 담는 list\n",
    "    # 이후 평균을 구함\n",
    "    roc_auc_list = []\n",
    "    \n",
    "    # 3개 kfold\n",
    "    kf = KFold(n_splits=3)\n",
    "    \n",
    "    # X_train을 다시 학습과 검증용 데이터로 분리\n",
    "    for tr_index, val_index in kf.split(X_train):\n",
    "        X_tr, y_tr = X_train.iloc[tr_index], y_train.iloc[tr_index]\n",
    "        X_val, y_val = X_train.iloc[val_index], y_train.iloc[val_index]\n",
    "        \n",
    "        # early stopping 설정 및 추출된 학습과 검증 데이터로 XGBClassifier 수행\n",
    "        lgbm_clf.fit(X_tr, y_tr, early_stopping_rounds=30, eval_metric='auc',\n",
    "                     eval_set = [(X_tr, y_tr), (X_val, y_val)])\n",
    "        score = roc_auc_score(y_val, lgbm_clf.predict_proba(X_val)[:, 1])\n",
    "        roc_auc_list.append(score)\n",
    "        \n",
    "    return -1 * np.mean(roc_auc_list)\n",
    "\n",
    "# 3. fmin (best 구하기)\n",
    "from hyperopt import fmin, tpe, Trials\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(fn=objective_func, space=lgbm_search_space,\n",
    "            max_evals=50, trials=trials,\n",
    "            rstate=np.random.default_rng(seed=30))\n",
    "print('best: ', best)\n",
    "\n",
    "# 4. 최적의 하이퍼 파라미터로 모델 구축\n",
    "lgbm_clf = LGBMClassifier(n_estimators=500,\n",
    "                          num_leaves = int(best['num_leaves']),\n",
    "                          max_depth = int(best['max_depth']),\n",
    "                          min_child_samples = int(best['min_child_samples']),\n",
    "                          subsample=round(best['subsample'], 5),\n",
    "                          learning_rate = round(best['learning_rate'], 5))\n",
    "# evaluation metric -> auc, early stopping -> 100\n",
    "lgbm_clf.fit(X_tr, y_tr, early_stopping_roudns=100, eval_metric='auc', eval_set = [(X_tr, y_tr), (X_val, y_val)])\n",
    "lgbm_roc_score = roc_auc_score(y_test, lgbm_clf.predict_proba(X_test)[:, 1])\n",
    "print('ROC AUC: {0: .4f}'.format(lgbm_roc_score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
